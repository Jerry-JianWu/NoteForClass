# 对比学习 Contrastive learning

对比学习是一种自监督学习类型（无监督学习），核心思想是区分物体之间的相似性与不相似性。例如下图，对于有监督学习的分类问题，我们希望分类模型能识别出来左侧是各种豹子，而右侧图片是其他事物。但对于无监督的对比学习来说，我们希望模型能够识别出前左侧图片是一个类别，右侧图片是另外的类别，也就是说对比学习不需要知道左侧图中内容是什么，只需要知道谁与谁相似，谁与谁不相似。在神经网络中，得到这些图片对应的特征向量时，我们希望左侧图的特征向量的距离比较近而与右侧图特征向量较远，也即，对比学习要达到的目标是所有相似的物体在特征空间相邻的区域，而不相似的物体都在不相邻的区域。
![](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304170950819.png)

那么问题来了，通过上述的描述，对比学习应该还是需要标签信息去做有监督的学习的？那么对比学习为什么会被认为是一种无监督的训练方式呢，这是因为人们可以使用**代理任务(pretext task)**来定义物体之间的相似性，即谁与谁相似，谁与谁又不相似，代理任务通常是认为设定的一些规则，这些规则定义了哪些图片相似，哪些图片不相似，从而提供了一个监督信号去训练模型，这就是所谓的自监督。



对比学习的典型范式：**代理任务+目标函数**，代理任务和目标函数也是对比学习与有监督学习最大的区别。
目标函数：是优化问题的一个概念，在机器学习问题中，通常是在一定约束条件下，以最大/最小这个函数为目标，最终求得对应的参数权重。目标函数是通过将损失函数和正则化函数相结合的得到的，主要目的是在训练过程中最小化损失函数的值，提高模型的泛化能力。
损失函数：损失函数是用来表示预测值与真实值之间差异的函数，衡量模型的输出和真实值之间的误差，用来评估模型的准确度。

有监督学习的流程，输入x，通过模型输出得到y，输出y和真实label(ground truth标注数据)通过损失函数计算损失，以此进行模型训练。
在无监督学习中，没有标注数据时，代理任务用来解决这个问题，用代理任务来定义对比学习的正负样本，无监督学习一旦有了输出y和真的label，就需要有目标函数来计算两者的损失。

## InstDisc论文(个体判别，每张图片自成一类)

提出了个体判别这个代理任务，而且用这个代理任务和NCEloss做对比学习，从而取得了不错的无监督表征学习的结果，同时它还提出了用别的数据结构存储这种大量的负样本，以及如何对特征进行动量式的更新。

CNN在这其中作为一个编码的作用

InstDisc论文中下图的前向过程：batch size=256张图片经过一个res50后维度变成2048维，之后降维再通过L2归一化后得到了128维的特征向量，memory bank中存放的是imagenet数据集中的特征向量，论文中随机抽取了4096个负样本和这一批次256张图片的正样本进行nce loss的学习，再将学习到的特征对memory bank中的向量进行更新，接下来反复这个过程使网络和memory bank中的学到的特征尽可能的有区分性（目的就是在于每个物体作为自己的类，要跟别的物体有区别，所以要有区分性）

![image-20230413110354329](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304131103350.png)

## Representation Learning with Contrastive Predictive Coding

![image-20230413160654253](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304131606281.png)

本文的想法：有一个输入x（一个持续的序列），t表示当前时刻t-i表示过去的时刻，t+i表示未来的时刻，然后把这些特征扔给一个编码器，这些编码器返回的特征再输出进一个自回归的模型（图中的gar部分，auto regression），一般常见的自回归模型，就是RNN或者LSTM的模型，所以每一步最后的输出，就会得到图中红色的方块(ct,context representation)，如果这个上下文的特征表示足够好（即它真的包含了当前和之前所有的这些信息），那么它应当时可以做出一些合理的预测的，可以用ct预测未来时刻的$z_{t+1},z_{t+2}$。
正负样本的定义方式：正样本是 未来的输入通过编码器以后得到的未来时刻的特征输出
负样本的定义很广泛，可以是任意选取 通过这个编码器得到的输出。

这个可以作为一个通用的结构应用在各种方面，例如图片可以是patch块从左上到右下形成一个序列。





## MoCo：Momentum Contrast for Unsupervised Visual Representation Learning

### 摘要

动量从数学上可以简单理解为加权移动平均
$$
y_t=m·y_{t-1}+(1-m)·x_t
$$
$y_t是当前时刻的输出，m是介乎于0到1之间的超参数，y_{t-1}是上一时刻的输出，x_t是这一时刻的输入$

也就是说这一时刻的输出不完全依赖于这一时刻的输入，也让上一时刻的输出来凑热闹，从式子可以看出当m趋近于1时这一时刻的输入改变应当是比较缓慢的，因为比较依赖上一时刻的输出，而当m趋近于0的时候，$y_t$就比较依赖于$x_t$。MoCo利用了这一特性来缓慢的更新编码器，从而让中间字典中学习的特征尽可能保证一致。

```
We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks.

我们提出了用于无监督视觉表征学习的动量对比度( MoCo )。从对比学习作为字典查找的角度出发，我们构建了一个带有队列和移动平均编码器的动态字典。这使得可以即时构建一个大的、一致的字典，从而促进对比无监督学习。MoCo在ImageNet分类的常用线性协议下提供了有竞争力的结果。更重要的是，MoCo学习到的表征很好地迁移到下游任务。

队列中的样本不需要做梯度回传，所以可以往其中放很多的负样本，使得这个字典变得很大
移动平均编码器是为了字典里的特征能够尽量保持一致
```

### 相关工作

MoCo把之前的对比学习方法归纳成 都在做同样一件事——做动态字典。下图为一般对比学习的框架

![image-20230414105053448](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304141050474.png)

$x_1分为两个部分进入编码器，编码器E_{11}和E_{12}可以相同，可以不同，f_{11}作为锚点anchor，\\而正样本和负样本都是相对锚点f_{11}来说的，且正负样本使用相同的编码器E_{12}$

$已经编码好的f_{11}当成query在下方由正负样本构成的字典key的特征条目中进行查找\\尽可能的匹配他的相似特征f_{12}，和其他的key远离。$

**所以MoCo的目的在于构造一个够大且一致的字典**

<img src="https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304141113011.png" alt="image-20230414111323978" style="zoom:50%;" />

- 字典的大小是成千上万的，显卡的内存是吃不消的，所以需要想一个办法将字典的大小和显卡内存的限制剥离开，让字典的大小和模型每次做前向传播的batch size大小分开。由此作者想到queue队列的数据结构。
- 队列可以很大，但每次做更新的时候是一点一点去做，现在的batch进入队列，最老的mini-batch移出队列，这样可以将训练的mini-batch的大小和训练的队列大小分开，因此字典queue的大小可以设置的很大。
- 那么下一个问题就是如何保持特征key的一致性，key是不同时刻的编码抽取得到的特征，所以引入了momentum encoder$encoder \to \theta_q,momentum\ encoder \to \theta_k=m·\theta_{k-1}+(1-m)·\theta_q $ 
- 由上式可知，可以设置一个较大的动量m来缓慢的更新$\theta_k$，这样就保证了字典中的key是由相似的编码器抽取得到的，也是基于这两个方法，MoCo可以构造一个又大又相似的字典

代理任务意味着所解决的任务并不是真正感兴趣的，而是仅仅为了学习一个好的数据表征的真正目的而解决的。这篇文章选择了个体判别。
代理任务的用处就是生成一个自监督的信号从而去充当ground truth标签信息。
损失函数可以独立于代理任务研究

```
Unsupervised/self-supervised1 learning methods generally involve two aspects: pretext tasks and loss functions. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation. Loss functions can often be investigated independently of pretext tasks. MoCo focuses on the loss function aspect. Next we discuss related studies with respect to these two aspects
```

### MoCo方法

```
Contrastive learning, and its recent developments, can be thought of as training an encoder for a dictionary look-up task, as described next.
```

对比学习以及它的发展，可以被认为是为字典查找任务训练编码器。

#### InfoNCE loss的提出思路

将一个编码好的 query q和一系列编码好的字典查询key：$\{k_0,k_1,k_2,...\}$。假设字典中存在一个匹配q的key，记为$k_+$(k postive)，对比学习的损失函数希望做到在q与它的匹配$k_+$相似和与其他查询key不相似时，对比函数的损失值很低(若与$k_+$不相似和key相似时就需要去更新模型)。

InfoNCE提出的数学过程
$Softmax \to CrossEntropy \to NCE \to InfoNCE$
$$
Softmax(z) = \frac{exp(z_+)}{\sum^k_{i=0}exp(z_i)}\\ \to CrossEntropy(z) = -log\frac{exp(z_+)}{\sum^k_{i=0}exp(z_i)}\\ \to InfoNCE = -log\frac{exp(q · k_+/ \tau)}{\sum^k_{i=0}exp(q · k_+/ \tau)}
$$
S和C中的k指的是有多少个类别，对比学习理论上可以使用交叉熵来计算损失，但理论上行不通，因为若要使用Instance Discrimination个体判别来作为代理任务的话，每一个物体都作为自己的类分类，k将会是一个巨大的数字，在Imagenet数据集上就有128万个类别，会带来巨大的计算复杂度。

NCE(noise contrastive entrpoy)的解决办法：因为上面的问题是因为类别过度而导致无法使用softmax，那么将问题简化为二分类问题，分类成数据类别(data sample)和噪声类别(noisy sample)，但计算复杂度还是没有降下来，那么如何将计算复杂度降下来呢，nce的办法是取近似，与其在整个数据集里用所有的负样本去计算，不如在数据集里面取一些负样本去计算，也就是这里estimation的含义。所以选取的负样本越多就会带来更好的效果，也是MoCo的构造大的字典的想法来源。

InfoNCE是NCE的一个变体，作者认为那么多的噪声类，他们很有可能不是一个类别，所以简单的进行二分类对模型的训练不是很友好，所以提出用多分类代替二分类。求和公式中的上标k指的是负样本的数量，q·k相当于softmax里面的$z_+$（logits），$\tau$是温度超参数，如果tau的值过大(会使分布过于平缓)，那么会导致对比损失对所有负样本都一视同仁，没有区分了；若tao的值过小(使分布非常集中，尖而窄)，又会让模型只会关注哪些特别困难的样本，但这些负样本很有可能是潜在的正样本，如果模型过于关注这些负样本则会导致模型很难收敛或者学好的特征不好去泛化。
求和公式是从0到k，也就是在1个正样本和k个负样本(字典里所有的key)做求和，所以infoNCE直观的感觉就是CrossEntropy，实际上做的任务是一个k+1的分类任务，就是想把q这个图片分类成$k_+$这个类。
模型的输入query和key分别是经过编码器得到的，至于模型到底是什么以及输入到底是什么，他们具体的实现由具体的代理任务决定，且编码器可以是一样的，也可以是不一样的，参数可以全部共享或者部分共享也可以完全不一样。

#### Momentum Contrast(再次强调研究动机)

（写作方式：每个部分都有承上启下的段落，让人了解脉络）

从之前的角度来看，对比学习是一种在图像等高维连续输入熵构建离散字典的方法。字典是动态的，因为字典里的key都是随机采样的，且编码器也是在训练过程中不断改变的，他们的假设是在一个包含丰富负样本的大型字典里可以学到好的特征，而字典中key的编码器在进化过程中尽可能保持一致。（一致性和大）
将字典看作是一个动态变化的队列，可以使得当前的mini-batch能够利用到之前batchcunchudekey，这样就把字典的大小和mini-batch的大小剥离开了，可以使用一个正常大小的mini-batch，但是字典的大小可以非常大(可以当作超参数进行设置，字典一直都是整个数据集的一个自己，因为是从数据集上抽取一部分数据来估计误差)，因为队列的先进先出的特性，可以最先移除最老的mini-batch计算的key。



#### Momentum update

使用队列可以使得字典变得很大，但也让反向传播更新编码器变得困难(梯度应该传播到队列中的所有样本，但队列很大)。文中提到一个简单的解决办法就是将query编码器更新的参数直接复制到key编码器这边来，但是作者做实验发现这种效果并不好，所以作者基于此提出了动量更新的方式。
$$
\theta_k \gets m·\theta_k+(1-m)\theta_q
$$
![f3](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304172002113.png)

若是简单的复制参数过来，假设mini-batch是1，则会导致整个字典中每个key的一致性很差，动量式更新时可以保证字典的一致性。



#### Relations to previous mechanisms

![f4](https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304172038509.png)

(a)end to end(端到端):这里端到端指的是编码器可以通过梯度实时回传更新，但也因为此，字典和mini-batch的大小一致，一个字典里存储的都是来自同一个mini-batch的key，所以每次输入的mini-batch就不能很大，但一致性很高。

(b)memory bank的方法:memory bank 存储所有数据的特征 ，再随机抽样一些特征key，encoder更新后接着用更新后的encoder更新memory bank中相应的被抽去的key值，这样导致每次抽取时使用的是不同的encoder，所以一致性很差。

(c)MoCo:对右侧分支改进，使用动量式更新，来构造了一个又大一致性又好的字典。

#### MoCo伪码

<img src="https://gitee.com/Jerry-wu-jian/img-load/raw/master/noteImg/202304172111891.png" alt="image-20230417211123854" style="zoom:50%;" />

```
初始化q和key的参数

随机抽样对抽样进行随机增强形成正样本对xq和xk（batch是256）

q和k进行前向传播，维度是256 x 128
k去掉梯度放入队列

计算postive的logits 256x1
和
negative的logits 256x65536，infoNCE的分子
进行拼接得到分母

设置全0的标签向量，因为在上一步中正样本一定是在位置0的，所以若找对了正样本key，在分类任务里得到的正确类别就是类别0

进行梯度回传，更新q的编码器

动量式更新

更新队列，新算的放进队列，老的移出队列
```

